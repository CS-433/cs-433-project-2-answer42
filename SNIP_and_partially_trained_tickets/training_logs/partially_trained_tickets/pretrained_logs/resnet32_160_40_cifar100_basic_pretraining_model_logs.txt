 => Using seed 2020
 => Using device: cuda
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar100/cifar-100-python.tar.gz
Extracting ./cifar100/cifar-100-python.tar.gz to ./cifar100
Files already downloaded and verified
=> Using a preset learning rate schedule:
{0: 0.1, 80: 0.010000000000000002, 120: 0.001}
Pretraining epoch 0: loss 0.0638409, train_acc 11.15%, test_acc 11.32%
Pretraining epoch 1: loss 0.0538594, train_acc 19.57%, test_acc 19.29%
Pretraining epoch 2: loss 0.0444462, train_acc 31.38%, test_acc 31.71%
Pretraining epoch 3: loss 0.0372091, train_acc 38.52%, test_acc 39.40%
Pretraining epoch 4: loss 0.0321567, train_acc 47.19%, test_acc 45.06%
Pretraining epoch 5: loss 0.0288631, train_acc 49.75%, test_acc 46.92%
Pretraining epoch 6: loss 0.0264485, train_acc 51.42%, test_acc 48.50%
Pretraining epoch 7: loss 0.0246393, train_acc 52.59%, test_acc 48.49%
Pretraining epoch 8: loss 0.0232103, train_acc 58.70%, test_acc 53.40%
Pretraining epoch 9: loss 0.0219646, train_acc 57.91%, test_acc 52.32%
Pretraining epoch 10: loss 0.0210417, train_acc 63.94%, test_acc 57.43%
Pretraining epoch 11: loss 0.0202459, train_acc 64.70%, test_acc 58.22%
Pretraining epoch 12: loss 0.0194747, train_acc 63.73%, test_acc 56.99%
Pretraining epoch 13: loss 0.0187396, train_acc 64.85%, test_acc 57.85%
Pretraining epoch 14: loss 0.0182590, train_acc 64.84%, test_acc 58.27%
Pretraining epoch 15: loss 0.0178030, train_acc 68.15%, test_acc 60.19%
Pretraining epoch 16: loss 0.0171855, train_acc 69.33%, test_acc 60.98%
Pretraining epoch 17: loss 0.0167124, train_acc 69.56%, test_acc 59.47%
Pretraining epoch 18: loss 0.0164395, train_acc 67.79%, test_acc 59.08%
Pretraining epoch 19: loss 0.0161420, train_acc 71.83%, test_acc 61.91%
Pretraining epoch 20: loss 0.0156533, train_acc 71.81%, test_acc 61.43%
Pretraining epoch 21: loss 0.0154102, train_acc 72.70%, test_acc 62.58%
Pretraining epoch 22: loss 0.0151070, train_acc 71.74%, test_acc 61.56%
Pretraining epoch 23: loss 0.0149336, train_acc 72.31%, test_acc 61.82%
Pretraining epoch 24: loss 0.0146581, train_acc 69.22%, test_acc 59.98%
Pretraining epoch 25: loss 0.0144308, train_acc 72.91%, test_acc 61.77%
Pretraining epoch 26: loss 0.0142096, train_acc 73.22%, test_acc 62.26%
Pretraining epoch 27: loss 0.0141104, train_acc 71.19%, test_acc 59.93%
Pretraining epoch 28: loss 0.0139377, train_acc 71.47%, test_acc 60.67%
Pretraining epoch 29: loss 0.0136978, train_acc 72.64%, test_acc 61.49%
Pretraining epoch 30: loss 0.0136710, train_acc 73.63%, test_acc 62.29%
Pretraining epoch 31: loss 0.0134390, train_acc 74.83%, test_acc 62.81%
Pretraining epoch 32: loss 0.0133836, train_acc 75.93%, test_acc 63.57%
Pretraining epoch 33: loss 0.0132585, train_acc 73.61%, test_acc 61.79%
Pretraining epoch 34: loss 0.0131029, train_acc 73.32%, test_acc 61.52%
Pretraining epoch 35: loss 0.0130196, train_acc 74.59%, test_acc 62.32%
Pretraining epoch 36: loss 0.0128845, train_acc 75.25%, test_acc 62.37%
Pretraining epoch 37: loss 0.0128321, train_acc 72.60%, test_acc 60.84%
Pretraining epoch 38: loss 0.0126928, train_acc 75.03%, test_acc 62.60%
Pretraining epoch 39: loss 0.0127292, train_acc 76.19%, test_acc 63.24%
Pretraining epoch 40: loss 0.0126316, train_acc 76.18%, test_acc 63.89%
Pretraining epoch 41: loss 0.0122881, train_acc 76.25%, test_acc 63.17%
Pretraining epoch 42: loss 0.0124452, train_acc 75.75%, test_acc 62.18%
Pretraining epoch 43: loss 0.0123835, train_acc 75.47%, test_acc 62.52%
Pretraining epoch 44: loss 0.0120421, train_acc 76.21%, test_acc 62.13%
Pretraining epoch 45: loss 0.0123397, train_acc 76.24%, test_acc 62.87%
Pretraining epoch 46: loss 0.0121168, train_acc 76.54%, test_acc 63.68%
Pretraining epoch 47: loss 0.0119285, train_acc 76.06%, test_acc 63.03%
Pretraining epoch 48: loss 0.0118892, train_acc 75.00%, test_acc 60.95%
Pretraining epoch 49: loss 0.0118833, train_acc 77.57%, test_acc 63.47%
Pretraining epoch 50: loss 0.0118126, train_acc 73.25%, test_acc 59.99%
Pretraining epoch 51: loss 0.0118226, train_acc 77.81%, test_acc 63.39%
Pretraining epoch 52: loss 0.0117387, train_acc 78.77%, test_acc 64.41%
Pretraining epoch 53: loss 0.0118585, train_acc 77.26%, test_acc 63.09%
Pretraining epoch 54: loss 0.0117920, train_acc 76.36%, test_acc 61.89%
Pretraining epoch 55: loss 0.0116383, train_acc 76.75%, test_acc 62.74%
Pretraining epoch 56: loss 0.0115200, train_acc 76.02%, test_acc 62.85%
Pretraining epoch 57: loss 0.0114291, train_acc 75.71%, test_acc 62.28%
Pretraining epoch 58: loss 0.0114643, train_acc 77.79%, test_acc 62.43%
Pretraining epoch 59: loss 0.0113978, train_acc 74.60%, test_acc 62.11%
Pretraining epoch 60: loss 0.0114695, train_acc 78.81%, test_acc 64.24%
Pretraining epoch 61: loss 0.0111486, train_acc 76.92%, test_acc 63.02%
Pretraining epoch 62: loss 0.0113570, train_acc 77.23%, test_acc 64.01%
Pretraining epoch 63: loss 0.0114133, train_acc 75.74%, test_acc 62.07%
Pretraining epoch 64: loss 0.0111536, train_acc 77.87%, test_acc 63.24%
Pretraining epoch 65: loss 0.0112955, train_acc 76.93%, test_acc 63.03%
Pretraining epoch 66: loss 0.0112814, train_acc 77.07%, test_acc 63.91%
Pretraining epoch 67: loss 0.0112185, train_acc 77.12%, test_acc 62.43%
Pretraining epoch 68: loss 0.0111931, train_acc 74.72%, test_acc 61.32%
Pretraining epoch 69: loss 0.0110854, train_acc 74.83%, test_acc 60.58%
Pretraining epoch 70: loss 0.0110738, train_acc 78.30%, test_acc 63.65%
Pretraining epoch 71: loss 0.0109747, train_acc 80.22%, test_acc 65.03%
Pretraining epoch 72: loss 0.0111039, train_acc 75.57%, test_acc 61.13%
Pretraining epoch 73: loss 0.0109260, train_acc 77.64%, test_acc 63.23%
Pretraining epoch 74: loss 0.0110164, train_acc 76.48%, test_acc 62.35%
Pretraining epoch 75: loss 0.0109251, train_acc 76.66%, test_acc 62.18%
Pretraining epoch 76: loss 0.0109872, train_acc 77.23%, test_acc 62.84%
Pretraining epoch 77: loss 0.0108773, train_acc 79.23%, test_acc 64.72%
Pretraining epoch 78: loss 0.0109757, train_acc 79.65%, test_acc 64.67%
Pretraining epoch 79: loss 0.0107682, train_acc 76.27%, test_acc 61.77%
Pretraining epoch 80: loss 0.0055746, train_acc 93.25%, test_acc 73.21%
Pretraining epoch 81: loss 0.0040156, train_acc 94.84%, test_acc 73.79%
Pretraining epoch 82: loss 0.0034228, train_acc 95.49%, test_acc 74.28%
Pretraining epoch 83: loss 0.0030410, train_acc 96.17%, test_acc 74.38%
Pretraining epoch 84: loss 0.0027112, train_acc 96.75%, test_acc 73.94%
Pretraining epoch 85: loss 0.0024773, train_acc 97.06%, test_acc 73.98%
Pretraining epoch 86: loss 0.0022739, train_acc 97.44%, test_acc 74.01%
Pretraining epoch 87: loss 0.0020758, train_acc 97.60%, test_acc 74.04%
Pretraining epoch 88: loss 0.0019368, train_acc 97.96%, test_acc 73.91%
Pretraining epoch 89: loss 0.0018232, train_acc 98.21%, test_acc 74.17%
Pretraining epoch 90: loss 0.0016794, train_acc 98.40%, test_acc 74.00%
Pretraining epoch 91: loss 0.0015552, train_acc 98.45%, test_acc 73.92%
Pretraining epoch 92: loss 0.0014758, train_acc 98.67%, test_acc 73.66%
Pretraining epoch 93: loss 0.0013518, train_acc 98.71%, test_acc 73.91%
Pretraining epoch 94: loss 0.0013309, train_acc 98.91%, test_acc 73.69%
Pretraining epoch 95: loss 0.0012626, train_acc 98.92%, test_acc 73.93%
Pretraining epoch 96: loss 0.0012001, train_acc 99.08%, test_acc 73.92%
Pretraining epoch 97: loss 0.0011151, train_acc 99.13%, test_acc 73.74%
Pretraining epoch 98: loss 0.0010834, train_acc 99.22%, test_acc 73.72%
Pretraining epoch 99: loss 0.0010716, train_acc 99.27%, test_acc 73.46%
Pretraining epoch 100: loss 0.0009818, train_acc 99.27%, test_acc 73.84%
Pretraining epoch 101: loss 0.0009380, train_acc 99.31%, test_acc 73.69%
Pretraining epoch 102: loss 0.0009163, train_acc 99.40%, test_acc 73.82%
Pretraining epoch 103: loss 0.0008736, train_acc 99.45%, test_acc 73.54%
Pretraining epoch 104: loss 0.0008441, train_acc 99.42%, test_acc 73.44%
Pretraining epoch 105: loss 0.0008268, train_acc 99.47%, test_acc 73.76%
Pretraining epoch 106: loss 0.0007901, train_acc 99.47%, test_acc 73.46%
Pretraining epoch 107: loss 0.0007797, train_acc 99.56%, test_acc 73.22%
Pretraining epoch 108: loss 0.0007458, train_acc 99.64%, test_acc 73.65%
Pretraining epoch 109: loss 0.0007267, train_acc 99.55%, test_acc 73.18%
Pretraining epoch 110: loss 0.0007186, train_acc 99.61%, test_acc 73.24%
Pretraining epoch 111: loss 0.0006724, train_acc 99.69%, test_acc 73.46%
Pretraining epoch 112: loss 0.0006478, train_acc 99.67%, test_acc 73.45%
Pretraining epoch 113: loss 0.0006757, train_acc 99.71%, test_acc 73.39%
Pretraining epoch 114: loss 0.0006608, train_acc 99.63%, test_acc 73.33%
Pretraining epoch 115: loss 0.0006479, train_acc 99.69%, test_acc 73.44%
Pretraining epoch 116: loss 0.0006272, train_acc 99.66%, test_acc 73.57%
Pretraining epoch 117: loss 0.0006235, train_acc 99.68%, test_acc 73.51%
Pretraining epoch 118: loss 0.0006054, train_acc 99.74%, test_acc 73.53%
Pretraining epoch 119: loss 0.0005832, train_acc 99.72%, test_acc 73.69%
Pretraining epoch 120: loss 0.0004861, train_acc 99.85%, test_acc 73.86%
Pretraining epoch 121: loss 0.0004435, train_acc 99.87%, test_acc 73.94%
Pretraining epoch 122: loss 0.0004188, train_acc 99.87%, test_acc 73.76%
Pretraining epoch 123: loss 0.0003949, train_acc 99.88%, test_acc 73.93%
Pretraining epoch 124: loss 0.0004066, train_acc 99.87%, test_acc 74.06%
Pretraining epoch 125: loss 0.0003742, train_acc 99.91%, test_acc 73.82%
Pretraining epoch 126: loss 0.0003697, train_acc 99.90%, test_acc 73.87%
Pretraining epoch 127: loss 0.0003601, train_acc 99.89%, test_acc 74.07%
Pretraining epoch 128: loss 0.0003595, train_acc 99.88%, test_acc 73.96%
Pretraining epoch 129: loss 0.0003630, train_acc 99.92%, test_acc 74.00%
Pretraining epoch 130: loss 0.0003598, train_acc 99.93%, test_acc 73.89%
Pretraining epoch 131: loss 0.0003435, train_acc 99.89%, test_acc 73.86%
Pretraining epoch 132: loss 0.0003548, train_acc 99.92%, test_acc 74.11%
Pretraining epoch 133: loss 0.0003398, train_acc 99.89%, test_acc 74.00%
Pretraining epoch 134: loss 0.0003429, train_acc 99.93%, test_acc 73.79%
Pretraining epoch 135: loss 0.0003445, train_acc 99.92%, test_acc 73.79%
Pretraining epoch 136: loss 0.0003497, train_acc 99.92%, test_acc 73.86%
Pretraining epoch 137: loss 0.0003315, train_acc 99.91%, test_acc 73.95%
Pretraining epoch 138: loss 0.0003298, train_acc 99.91%, test_acc 73.69%
Pretraining epoch 139: loss 0.0003369, train_acc 99.93%, test_acc 73.87%
Pretraining epoch 140: loss 0.0003217, train_acc 99.91%, test_acc 73.65%
Pretraining epoch 141: loss 0.0003122, train_acc 99.93%, test_acc 74.09%
Pretraining epoch 142: loss 0.0003292, train_acc 99.96%, test_acc 73.74%
Pretraining epoch 143: loss 0.0003204, train_acc 99.93%, test_acc 73.86%
Pretraining epoch 144: loss 0.0003222, train_acc 99.94%, test_acc 73.88%
Pretraining epoch 145: loss 0.0003165, train_acc 99.94%, test_acc 73.70%
Pretraining epoch 146: loss 0.0003216, train_acc 99.91%, test_acc 74.01%
Pretraining epoch 147: loss 0.0003051, train_acc 99.90%, test_acc 73.87%
Pretraining epoch 148: loss 0.0003050, train_acc 99.93%, test_acc 73.69%
Pretraining epoch 149: loss 0.0003132, train_acc 99.95%, test_acc 73.52%
Pretraining epoch 150: loss 0.0003171, train_acc 99.93%, test_acc 74.07%
Pretraining epoch 151: loss 0.0003131, train_acc 99.92%, test_acc 74.02%
Pretraining epoch 152: loss 0.0003098, train_acc 99.93%, test_acc 73.53%
Pretraining epoch 153: loss 0.0003005, train_acc 99.95%, test_acc 73.80%
Pretraining epoch 154: loss 0.0002877, train_acc 99.94%, test_acc 73.87%
Pretraining epoch 155: loss 0.0002976, train_acc 99.94%, test_acc 73.85%
Pretraining epoch 156: loss 0.0002989, train_acc 99.94%, test_acc 73.81%
Pretraining epoch 157: loss 0.0003077, train_acc 99.96%, test_acc 74.03%
Pretraining epoch 158: loss 0.0003017, train_acc 99.94%, test_acc 73.99%
Pretraining epoch 159: loss 0.0003056, train_acc 99.95%, test_acc 73.57%
 => Saving model at '/content/drive/MyDrive/pruningData/partially_trained/pretrained_models/resnet32_160_40_cifar100_basic_pretraining_model.pt'
